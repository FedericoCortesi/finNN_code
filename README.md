# finNN_code: Financial Neural Network Training Pipeline

A neural network training pipeline for financial time series prediction using walk-forward cross-validation. This repository implements a framework for training and evaluating machine learning models on financial data with proper temporal validation and hyperparameter optimization.

## ğŸ—ï¸ Project Structure

```
finNN_code/
â”œâ”€â”€ src/                              # Main source code
â”‚   â”œâ”€â”€ config/                       # Configuration files and types
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config_types.py          # Configuration dataclasses
â”‚   â”‚   â”œâ”€â”€ default.yaml             # Default experiment configuration
â”‚   â”‚   â”œâ”€â”€ debug.yaml               # Debug configuration
â”‚   â”‚   â””â”€â”€ search_debug.yaml        # Hyperparameter search debug config
â”‚   â”œâ”€â”€ data/                        # Data files and analysis
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_analysis.ipynb      # Exploratory data analysis
â”‚   â”‚   â”œâ”€â”€ get_data.ipynb           # Data acquisition notebook
â”‚   â”‚   â”œâ”€â”€ permnos_info.csv         # S&P 500 company metadata
â”‚   â”‚   â”œâ”€â”€ permons_list.txt         # List of company identifiers
â”‚   â”‚   â””â”€â”€ sp500_daily_data.parquet # S&P 500 daily price data
â”‚   â”œâ”€â”€ hyperparams_search/         # Hyperparameter optimization
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ search_utils.py          # Optuna-based hyperparameter search
â”‚   â”œâ”€â”€ models/                      # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ mlp.py                   # Multi-layer perceptron implementation
â”‚   â”œâ”€â”€ pipeline/                    # Data processing and validation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py         # Data preprocessing utilities
â”‚   â”‚   â”œâ”€â”€ walkforward.py          # Walk-forward cross-validation engine
â”‚   â”‚   â””â”€â”€ wf_config.py            # Walk-forward configuration dataclass
â”‚   â”œâ”€â”€ price_prediction/           # Price prediction experiments
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ visuals.ipynb           # Results visualization
â”‚   â”‚   â”œâ”€â”€ benchmarks/             # Baseline model implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ regressions.ipynb   # Linear regression benchmarks
â”‚   â”‚   â”œâ”€â”€ experiments/            # Experiment results and saved models
â”‚   â”‚   â”‚   â”œâ”€â”€ exp_001_20251011_162012_mlp/
â”‚   â”‚   â”‚   â”œâ”€â”€ exp_002_20251012_165113_pippo/
â”‚   â”‚   â”‚   â””â”€â”€ ...                 # Other experiment directories
â”‚   â”‚   â””â”€â”€ legacy/                 # Legacy training code
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ debug.py
â”‚   â”‚       â”œâ”€â”€ training_models.py
â”‚   â”‚       â””â”€â”€ training_pipeline.ipynb
â”‚   â”œâ”€â”€ training_routine/           # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ trainer.py              # Model training orchestrator
â”‚   â”‚   â””â”€â”€ metrics.py              # Training metrics computation
â”‚   â”œâ”€â”€ utils/                      # Utility functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ custom_formatter.py     # Custom logging formatters
â”‚   â”‚   â”œâ”€â”€ gpu_test.py             # GPU availability testing
â”‚   â”‚   â”œâ”€â”€ logging_utils.py        # Experiment logging utilities
â”‚   â”‚   â””â”€â”€ paths.py                # Path management
â”‚   â”œâ”€â”€ debug.ipynb                 # Main debugging notebook
â”‚   â””â”€â”€ run_experiments.py          # Main experiment runner script
â”œâ”€â”€ logs/                           # SLURM job logs
â”‚   â”œâ”€â”€ slurm_*.out                 # SLURM stdout logs
â”‚   â””â”€â”€ slurm_*.err                 # SLURM stderr logs
â”œâ”€â”€ train_job.sh                    # SLURM job submission script
â”œâ”€â”€ pyproject.toml                  # Project dependencies and metadata
â”œâ”€â”€ ssh_guide.md                    # SSH connection guide
â””â”€â”€ README.md                       # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python â‰¥ 3.12
- PyTorch (for GPU training)
- CUDA-capable GPU (recommended)

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd finNN_code
   ```

2. **Install the package:**
   ```bash
   pip install -e .
   ```

3. **Verify GPU setup:**
   ```bash
   python src/utils/gpu_test.py
   ```

### Running Your First Experiment

1. **Basic experiment with default settings:**
   ```bash
   python src/run_experiments.py --config default.yaml
   ```

2. **Debug experiment (smaller dataset, fewer epochs):**
   ```bash
   python src/run_experiments.py --config debug.yaml
   ```

3. **Custom experiment name:**
   ```bash
   python src/run_experiments.py --config default.yaml --exp-name my_first_experiment
   ```

4. **Hyperparameter search:**
   ```bash
   python src/run_experiments.py --config search_debug.yaml
   ```

## ğŸ“Š Walk-Forward Cross-Validation

The core of this framework is the walk-forward cross-validation system, which ensures proper temporal validation for time series data:

### How It Works

1. **Data Splitting**: The time series is divided into overlapping windows
2. **Temporal Ordering**: Training always occurs on past data, validation on recent past, testing on future
3. **Rolling Windows**: The validation window moves forward in time with each fold
4. **No Look-Ahead Bias**: Strict temporal ordering prevents data leakage

### Configuration Parameters

```yaml
walkforward:
  max_folds: 20              # Maximum number of folds (null = all possible)
  step: 251                  # Days to advance between folds (1 year â‰ˆ 251 trading days)
  ratio_train: 3             # Training period ratio
  ratio_val: 1               # Validation period ratio  
  ratio_test: 1              # Test period ratio
  lags: 20                   # Number of lagged features to use
  T: null                    # Total periods (auto-calculated from data if null)
```

**How it Works:**
The system automatically calculates `T_train`, `T_val`, and `T_test` based on the total available time periods and the specified ratios. Each fold advances by `step` periods while maintaining the same window sizes.

## âš™ï¸ Configuration System

Experiments are configured using YAML files in `src/config/`. The configuration system supports:

### Core Configuration Sections

```yaml
# Experiment metadata
experiment:
  name: "mlp"                        # Experiment identifier
  random_state: 1234                 # Random seed for reproducibility
  monitor: "val_loss"                # Metric to monitor for early stopping
  mode: "min"                        # Optimization direction (min/max)
  hyperparams_search: false          # Enable hyperparameter optimization

# Data configuration
data:
  df_path: null                      # Path to data file (auto-loads if null)
  target_col: "y"                    # Target variable column name
  feature_cols: ["feature_0", "feature_1", ...]  # Feature column names
  standardize: true                  # Apply standardization
  per_asset_norm: true               # Normalize per asset independently

# Model architecture
model:
  name: "mlp"                        # Model type (mlp, cnn1d, etc.)
  hparams:
    hidden_sizes: [32, 32]           # Hidden layer dimensions
    dropout_rate: 0.2                # Dropout rate
    activation: "relu"               # Activation function
    l2_reg: 0.001                    # L2 regularization strength
    output_activation: null          # Output layer activation

# Training parameters
trainer:
  params:
    epochs: 50                       # Maximum training epochs
    batch_size: 128                  # Training batch size
    lr: 1.0e-3                       # Learning rate
    loss: "mse"                      # Loss function
    metrics: ["mae", "mse"]          # Evaluation metrics
```

## ğŸ§  Model Architecture

### Available Models

Currently implemented:
- **MLP**: Multi-layer perceptron with configurable layers, dropout, and regularization

### Model Features

- **Configurable Architecture**: Variable number of hidden layers and neurons
- **Regularization**: Dropout and L2 regularization support
- **Flexible Activations**: ReLU, tanh, sigmoid, and other standard activations
- **Time Series Adaptation**: Designed for lagged feature inputs from financial data

### Adding New Models

1. Add model implementation to `src/models/`
2. Update the `create_model` function to handle your new model type
3. Configure model parameters in YAML files

## ğŸ”¬ Experiment Management

### Running Experiments

The experiment system automatically:

- **Creates unique experiment directories** with timestamps
- **Saves model checkpoints** for each fold
- **Logs comprehensive metrics** (RMSE, directional accuracy, etc.)
- **Tracks hyperparameters** and configuration
- **Enables experiment reproduction** with saved configs

### Results Structure

```
src/price_prediction/experiments/exp_XXX_YYYYMMDD_HHMMSS_name/
â”œâ”€â”€ trial_000/                       # Trial directory for hyperparameter search
â”‚   â”œâ”€â”€ fold_000/                    # Fold-specific results
â”‚   â”‚   â”œâ”€â”€ model_best.pth           # Best model checkpoint
â”‚   â”‚   â””â”€â”€ training_log.json        # Training metrics and losses
â”‚   â””â”€â”€ fold_001/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ config_snapshot.json             # Experiment configuration
â””â”€â”€ results.csv                      # Aggregated results across folds
```

### Results Analysis

Each experiment automatically saves:
- **Model Checkpoints**: Best model weights per fold
- **Training Logs**: Loss curves, metrics, and training progress
- **Configuration**: Complete experiment setup for reproducibility
- **Results CSV**: Performance metrics aggregated across folds

## ğŸ–¥ï¸ High-Performance Computing

### SLURM Integration

For cluster environments, use the provided SLURM script:

```bash
# Submit job to SLURM
sbatch train_job.sh

# Monitor job status
squeue -u $USER

# Check logs
tail -f logs/slurm_<job_id>.out
tail -f logs/slurm_<job_id>.err
```

The `train_job.sh` script:
- Requests H200 GPU resources (configurable)
- Loads miniforge/conda environment
- Sets up proper CUDA paths
- Runs experiments with logging
- Saves all outputs to `logs/` directory

### GPU Utilization

The framework uses PyTorch for GPU training:
- Automatic GPU detection via `gpu_test.py`
- Efficient data loading with multiple workers
- Memory-optimized batch processing
- Support for mixed precision training

## ğŸ“ˆ Performance Monitoring

### Logging System

The framework includes comprehensive logging via `ExperimentLogger`:

- **Structured Logging**: Automatic directory creation with timestamps
- **Training Metrics**: Loss curves, validation performance tracking
- **Model Checkpointing**: Automatic saving of best models per fold
- **Configuration Snapshots**: Complete reproducibility information

### Hyperparameter Optimization

Built-in Optuna integration for automated hyperparameter search:

```yaml
experiment:
  hyperparams_search: true           # Enable search
  n_trials: 50                       # Number of trials
  mode: "min"                        # Optimization direction

# Define search spaces in configuration
search_spaces:
  model.hparams.hidden_sizes: [[32], [64], [32, 32], [64, 32]]
  trainer.params.lr: [1e-4, 1e-3, 1e-2]
  trainer.params.batch_size: [128, 256, 512]
```

## ğŸ”§ Advanced Usage

### Custom Data Sources

The framework can work with custom financial datasets:

1. **Data Format**: Long-format DataFrame with:
   - `permno`: Asset identifier  
   - `t`: Time index (integer)
   - Feature columns (as specified in config)
   - Target column (`y` by default)

2. **Loading Custom Data:**
   ```bash
   python src/run_experiments.py --config default.yaml --data path/to/your/data.parquet
   ```

### Debugging and Development

1. **Debug Configuration**: Use smaller datasets and fewer epochs
   ```bash
   python src/run_experiments.py --config debug.yaml
   ```

2. **Jupyter Notebooks**: Interactive analysis available:
   - `src/debug.ipynb`: Main debugging notebook
   - `src/data/data_analysis.ipynb`: Data exploration
   - `src/price_prediction/visuals.ipynb`: Results visualization

### Extending the Framework

1. **Add New Models**: Extend `src/models/` with new architectures
2. **Custom Metrics**: Modify `src/training_routine/metrics.py`
3. **New Preprocessing**: Extend `src/pipeline/preprocessing.py`

## ğŸ§ª Development and Testing

### Interactive Development

```bash
# Main debugging notebook
jupyter notebook src/debug.ipynb

# Data exploration and analysis  
jupyter notebook src/data/data_analysis.ipynb
jupyter notebook src/data/get_data.ipynb

# Results visualization
jupyter notebook src/price_prediction/visuals.ipynb

# Benchmark comparisons
jupyter notebook src/price_prediction/benchmarks/regressions.ipynb
```

### Code Structure

- **Configuration-Driven**: YAML-based experiment setup with typed configurations
- **Modular Pipeline**: Separate components for data, models, training, and evaluation  
- **Walk-Forward Validation**: Proper temporal validation for financial time series
- **Experiment Tracking**: Automatic logging and reproducibility

## ğŸ“š Key Concepts

### Walk-Forward Validation vs Traditional CV

Traditional cross-validation randomly splits data, which can cause **look-ahead bias** in time series. Walk-forward validation:

- âœ… Respects temporal ordering
- âœ… Simulates real-world deployment
- âœ… Provides realistic performance estimates
- âœ… Prevents data leakage

### Financial Time Series Considerations

- **Stationarity**: Markets change over time
- **Regime Changes**: Model performance varies across market conditions  
- **Transaction Costs**: Real-world implementation considerations
- **Risk Management**: Drawdown and volatility controls

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Troubleshooting

### Common Issues

1. **GPU Not Detected**:
   ```bash
   python src/utils/gpu_test.py
   # Should show CUDA availability and GPU information
   ```

2. **NaN Metrics in Results**:
   - Check for empty test sets in walk-forward validation
   - Verify data has sufficient non-NaN values for each window
   - Review `T`, `step`, and `lags` parameters in walk-forward config

3. **High CPU Usage During Training**:
   - Increase `batch_size` for better GPU utilization
   - Use multiple workers in data loading
   - Check if data preprocessing is done on CPU vs GPU

4. **SLURM Job Failures**:
   - Check logs: `cat logs/slurm_<job_id>.err`
   - Verify conda environment and module loading
   - Ensure sufficient time and memory allocation

5. **Configuration Errors**:
   - Validate YAML syntax
   - Check that all required fields are present
   - Use `debug.yaml` for testing configuration changes

### Performance Tips

- **Start Small**: Use `debug.yaml` for initial testing
- **Monitor Resources**: Check GPU utilization with `nvidia-smi`
- **Batch Size**: Larger batches (512-2048) often improve GPU efficiency
- **Data Loading**: Use sufficient workers to avoid CPU bottlenecks

### Getting Help

- Check experiment logs for detailed error messages
- Use the debugging notebooks for interactive troubleshooting
- Review configuration examples in `src/config/`

---

**Happy Training! ğŸš€**