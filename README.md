# finNN_code: Financial Neural Network Training Pipeline

A neural network training pipeline for financial time series prediction using walk-forward cross-validation. This repository implements a framework for training and evaluating machine learning models on financial data with proper temporal validation and hyperparameter optimization.

## ğŸš€ Quick Start â€” Run Experiments

**To run experiments and generate forecasts:**

Open and run the notebook at:
```
src/benchmarks/forecast.ipynb
```

This Jupyter notebook contains the complete experimental pipeline and will generate predictions on financial time series data.

### Setup

1. **Create a Python environment (Python >= 3.12):**

```bash
python -m venv .venv
source .venv/bin/activate
```

2. **Install the package with dependencies:**

```bash
pip install -e .
```

3. **Run the forecast notebook:**
   - Open `src/benchmarks/forecast.ipynb` in Jupyter or VS Code
   - Run all cells to execute the full pipeline

## ğŸ—ï¸ Project Structure

```
finNN_code/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”‚   â””â”€â”€ forecast.ipynb           # â­ Main notebook â€” run experiments here
â”‚   â”œâ”€â”€ config/                      # Configuration files and types
â”‚   â”‚   â”œâ”€â”€ config_types.py
â”‚   â”‚   â”œâ”€â”€ default.yaml
â”‚   â”‚   â”œâ”€â”€ debug.yaml
â”‚   â”‚   â””â”€â”€ search_debug.yaml
â”‚   â”œâ”€â”€ data/                        # Data files and analysis
â”‚   â”‚   â”œâ”€â”€ data_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ get_data.ipynb
â”‚   â”‚   â”œâ”€â”€ permnos_info.csv
â”‚   â”‚   â”œâ”€â”€ permons_list.txt
â”‚   â”‚   â””â”€â”€ sp500_daily_data.parquet
â”‚   â”œâ”€â”€ hyperparams_search/          # Hyperparameter optimization (Optuna)
â”‚   â”‚   â””â”€â”€ search_utils.py
â”‚   â”œâ”€â”€ models/                      # Neural network architectures
â”‚   â”‚   â””â”€â”€ mlp.py
â”‚   â”œâ”€â”€ pipeline/                    # Data processing & walk-forward logic
â”‚   â”œâ”€â”€ price_prediction/            # Experiment outputs & benchmarks
â”‚   â”œâ”€â”€ training_routine/            # Trainer and metric computations
â”‚   â”œâ”€â”€ utils/                       # Logging, GPU checks, helpers
â”‚   â””â”€â”€ volatility/                  # Volatility analysis
â”œâ”€â”€ logs/                            # SLURM job logs
â”œâ”€â”€ train_job.slurm                  # Example SLURM submission script
â”œâ”€â”€ pyproject.toml                   # Project metadata & dependencies
â”œâ”€â”€ ssh_guide.md                     # Remote machine setup guide
â””â”€â”€ README.md                        # This file
```

## âš™ï¸ Configuration

Experiment settings are defined in YAML files under `src/config/`:

- **`default.yaml`** â€” Production experiment configuration
- **`debug.yaml`** â€” Quick debug runs with smaller datasets
- **`search_debug.yaml`** â€” Hyperparameter search configuration

Each config defines:
- `experiment` â€” experiment name and metadata
- `data` â€” data source and preprocessing
- `model` â€” neural network architecture and hyperparameters
- `trainer` â€” training loop settings (epochs, batch size, learning rate)
- `walkforward` â€” walk-forward cross-validation parameters

### Walk-Forward Cross-Validation

The pipeline uses rolling (walk-forward) windows to avoid look-ahead bias:
- Trains on past data
- Validates on intermediate periods
- Tests on future intervals

## ğŸ“Š Running Experiments from Command Line

If you prefer command-line execution instead of the notebook:

```bash
# Quick debug run
python src/run_experiments.py --config src/config/debug.yaml

# Full experiment
python src/run_experiments.py --config src/config/default.yaml

# Hyperparameter search
python src/run_experiments.py --config src/config/search_debug.yaml
```

Experiment results are saved to `src/price_prediction/experiments/` with timestamped folders containing:
- `config_snapshot.json` â€” Exact config used
- `results.csv` â€” Aggregated metrics
- `trial_*/fold_*/model_best.pth` â€” Trained model checkpoints
- `trial_*/fold_*/training_log.json` â€” Loss/metric logs per epoch

## ğŸ–¥ï¸ GPU Support

To verify GPU availability:
```bash
python src/utils/gpu_test.py
```

## ğŸš€ Running on HPC Clusters (SLURM)

Use the provided `train_job.slurm` script as a starting point:

```bash
sbatch train_job.slurm
squeue -u $USER
tail -f logs/slurm_<job_id>.err
```

Customize resources and module loads as needed for your cluster.

## ğŸ”§ Development & Debugging

- Interactive notebooks for exploration: `src/debug.ipynb`, `src/data/data_analysis.ipynb`
- Use `debug.yaml` for faster iteration with smaller configs
- Check GPU: `python src/utils/gpu_test.py`
- Inspect logs: `tail -f logs/slurm_<job_id>.err`

## ğŸ“¦ Dependencies

Key packages (see `pyproject.toml`):
- **PyTorch** â€” Neural networks
- **TensorFlow** â€” Deep learning
- **scikit-learn** â€” Machine learning utilities
- **Optuna** â€” Hyperparameter optimization
- **pandas, numpy** â€” Data processing
- **statsmodels** â€” Statistical modeling
- **tqdm** â€” Progress bars
- **PyYAML** â€” Configuration files

---

**Happy experimenting!** ğŸš€

For setup on remote machines, see `ssh_guide.md`.
