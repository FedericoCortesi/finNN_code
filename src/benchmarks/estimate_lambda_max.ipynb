{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1e75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from config.config_types import AppConfig\n",
    "from models import create_model\n",
    "from pipeline.walkforward import WFCVGenerator\n",
    "from utils.paths import CONFIG_DIR, SP500COPY_PATH, SP500_PATH, DATA_DIR, VOL_EXPERIMENTS_DIR, PRICE_EXPERIMENTS_DIR\n",
    "\n",
    "DEVICE = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab269d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "def _infer_lstm_input_size_from_ckpt(state_dict: dict) -> int | None:\n",
    "    \"\"\"\n",
    "    Efficiently finds LSTM input size from weights without hardcoded layer names.\n",
    "    searches for any parameter ending in 'weight_ih_l0' (input-hidden weights of layer 0).\n",
    "    \"\"\"\n",
    "    # Use a generator to find the first match without iterating the whole dict twice\n",
    "    for k, v in state_dict.items():\n",
    "        if \"weight_ih_l0\" in k: # Standard PyTorch LSTM parameter naming\n",
    "            # shape is [4 * Hidden, Input_Size]\n",
    "            return int(v.shape[1])\n",
    "    return None\n",
    "\n",
    "def _get_data_dims(X_sample: torch.Tensor | np.ndarray):\n",
    "    \"\"\"Normalized extractor for T (time) and D (features).\"\"\"\n",
    "    shape = X_sample.shape\n",
    "    if len(shape) == 2: # (N, T) -> Univariate\n",
    "        return shape[1], 1\n",
    "    elif len(shape) == 3: # (N, T, D) -> Multivariate\n",
    "        return shape[1], shape[2]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected data shape: {shape}. Expected (N, T) or (N, T, D).\")\n",
    "\n",
    "def _make_input_shape_for_eval(cfg, X_sample, state_dict):\n",
    "    name = cfg.model.name.lower()\n",
    "    T, D_data = _get_data_dims(X_sample)\n",
    "\n",
    "    if name in [\"lstm\", \"transformer\", \"gru\"]:\n",
    "        # Try to trust the checkpoint's reality over the data's shape\n",
    "        D_ckpt = _infer_lstm_input_size_from_ckpt(state_dict)\n",
    "        D = D_ckpt if D_ckpt is not None else D_data\n",
    "        \n",
    "        # Guardrail: Warn if mismatch\n",
    "        if D_ckpt is not None and D_ckpt != D_data:\n",
    "            print(f\"Warning: Data feature dim ({D_data}) != Model checkpoint dim ({D_ckpt}). Using Checkpoint dim.\")\n",
    "        \n",
    "        return (T, D)\n",
    "        \n",
    "    elif name == \"simplecnn\":\n",
    "        return (1, T) # (Channels, Length)\n",
    "        \n",
    "    elif name == \"mlp\":\n",
    "        return (T * D_data,) # Flattened\n",
    "        \n",
    "    else:\n",
    "        # Fallback or strict error\n",
    "        raise ValueError(f\"Unknown model name for shape inference: {name}\")\n",
    "\n",
    "def load_nn(base_path, X_test, fold_idx=0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads model safely by staging weights on CPU to avoid VRAM fragmentation.\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    # 1. Load Config\n",
    "    cfg_path = base_path / \"config_snapshot.json\"\n",
    "    if not cfg_path.exists():\n",
    "        # Fallback for some directory structures\n",
    "        cfg_path = base_path / \"config.json\" \n",
    "    \n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Config not found in {base_path}\")\n",
    "        \n",
    "    cfg_json = json.loads(cfg_path.read_text())\n",
    "    cfg = AppConfig.from_dict(cfg_json[\"cfg\"])\n",
    "\n",
    "    # 2. Path Validation\n",
    "    ckpt_path = base_path / f\"fold_{fold_idx:03d}\" / \"model_best.pt\"\n",
    "    if not ckpt_path.exists():\n",
    "        return None\n",
    "\n",
    "    # 3. Memory-Safe Load (The Fix for OOM)\n",
    "    # map_location='cpu' ensures we don't spike VRAM loading optimizer states\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    # Handle both full checkpoints and weight-only saves\n",
    "    if \"model_state\" in checkpoint:\n",
    "        raw_state = checkpoint[\"model_state\"]\n",
    "    elif \"state_dict\" in checkpoint:\n",
    "        raw_state = checkpoint[\"state_dict\"]\n",
    "    else:\n",
    "        raw_state = checkpoint # Assume it is just the weights\n",
    "\n",
    "    # Clean keys (remove torch.compile prefixes)\n",
    "    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in raw_state.items()}\n",
    "    \n",
    "    # 4. Infer Shapes using the cleaner dictionary\n",
    "    input_shape = _make_input_shape_for_eval(cfg, X_test, state_dict)\n",
    "    \n",
    "    # Handle Walkforward config optionally\n",
    "    lookback = getattr(cfg.walkforward, 'lookback', None)\n",
    "    output_shape = lookback + 1 if lookback is not None else 1\n",
    "\n",
    "    # 5. Critical Cleanup\n",
    "    # Delete the full checkpoint (which might hold Optimizer state) BEFORE model creation\n",
    "    del checkpoint\n",
    "    del raw_state\n",
    "    gc.collect()\n",
    "\n",
    "    # 6. Model Creation & Loading\n",
    "    # Create model on CPU first (default)\n",
    "    model = create_model(cfg.model, input_shape, output_shape)\n",
    "    \n",
    "    # Load weights (Strict checking ensures architecture matches weights)\n",
    "    try:\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Strict loading failed for {base_path.name}. Retrying with strict=False.\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402f6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def estimate_lambda_max_optimized(model, criterion, batch_tuple, n_iter=50, batch_size=32, device='cuda', tol=1e-3):\n",
    "    \"\"\"\n",
    "    Optimized Power Iteration for Lambda Max.\n",
    "    Handles memory aggressively to prevent OOM on LSTMs.\n",
    "    \"\"\"\n",
    "    # 1. Setup & Memory Prep\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # 2. Efficient Data Loading & Slicing\n",
    "    # CRITICAL: Slice BEFORE moving to GPU to save memory\n",
    "    inputs_raw, targets_raw = batch_tuple\n",
    "    \n",
    "    if len(inputs_raw) > batch_size:\n",
    "        inputs_raw = inputs_raw[:batch_size]\n",
    "        targets_raw = targets_raw[:batch_size]\n",
    "\n",
    "    # Robust Conversion (Numpy -> Tensor -> Float -> Device)\n",
    "    if isinstance(inputs_raw, np.ndarray):\n",
    "        inputs = torch.from_numpy(inputs_raw)\n",
    "    else:\n",
    "        inputs = inputs_raw\n",
    "        \n",
    "    if isinstance(targets_raw, np.ndarray):\n",
    "        targets = torch.from_numpy(targets_raw)\n",
    "    else:\n",
    "        targets = targets_raw\n",
    "\n",
    "    inputs = inputs.float().to(device)\n",
    "    targets = targets.float().to(device)\n",
    "\n",
    "    # 3. Initialize Eigenvector Estimate (v)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    v = [torch.randn_like(p) for p in params]\n",
    "    \n",
    "    # Normalize v\n",
    "    v_norm = torch.sqrt(sum(torch.sum(x * x) for x in v))\n",
    "    v = [x / v_norm for x in v]\n",
    "\n",
    "    current_lambda = 0.0\n",
    "    \n",
    "    try:\n",
    "        for i in range(n_iter):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # --- HVP Step ---            \n",
    "            # A. Forward Pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # B. First Backward\n",
    "            grads = torch.autograd.grad(loss, params, create_graph=True, only_inputs=True)\n",
    "            \n",
    "            # C. Gradient-Vector Product\n",
    "            grad_v_prod = sum(torch.sum(g * x) for g, x in zip(grads, v))\n",
    "            \n",
    "            # D. Second Backward (H*v)\n",
    "            Hv = torch.autograd.grad(grad_v_prod, params, retain_graph=False, only_inputs=True)\n",
    "            \n",
    "            # --- Power Iteration Update ---\n",
    "            hv_norm = torch.sqrt(sum(torch.sum(x * x) for x in Hv))\n",
    "            v = [x / hv_norm for x in Hv]\n",
    "            \n",
    "            # Convergence Check\n",
    "            prev_lambda = current_lambda\n",
    "            current_lambda = hv_norm.item()\n",
    "            \n",
    "            if i > 0 and abs(current_lambda - prev_lambda) < tol:\n",
    "                break\n",
    "\n",
    "            # CRITICAL: Delete graph variables immediately to free VRAM for next iter\n",
    "            del loss, outputs, grads, grad_v_prod, Hv\n",
    "            \n",
    "    finally:\n",
    "        # 4. Aggressive Cleanup\n",
    "        # Ensure we don't leave massive tensors hanging if the loop crashes\n",
    "        del inputs, targets, v, params\n",
    "        if 'loss' in locals(): del loss\n",
    "        if 'outputs' in locals(): del outputs\n",
    "        if 'grads' in locals(): del grads\n",
    "        \n",
    "        # Force Python GC and PyTorch Cache Clear\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return current_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4e8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg_mlp = AppConfig.from_dict(f\"{CONFIG_DIR}/vol_mlp.yaml\")\n",
    "cfg_cnn = AppConfig.from_dict(f\"{CONFIG_DIR}/vol_cnn.yaml\")\n",
    "cfg_lstm = AppConfig.from_dict(f\"{CONFIG_DIR}/vol_lstm.yaml\")\n",
    "cfg_transformer = AppConfig.from_dict(f\"{CONFIG_DIR}/vol_transformer.yaml\")\n",
    "\n",
    "wf = WFCVGenerator(cfg_lstm.walkforward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5403af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "\n",
    "\n",
    "for i, fold_data in enumerate(wf.folds()):\n",
    "    size = fold_data[4]\n",
    "    y_test = fold_data[5]\n",
    "\n",
    "    data[i] = fold_data\n",
    "    print(i)\n",
    "    #if i == fold_num:\n",
    "    #    break\n",
    "\n",
    "use = data[0]\n",
    "\n",
    "\n",
    "Xtr             = use[0]\n",
    "ytr             = use[1]\n",
    "Xv              = use[2]\n",
    "yv              = use[3]\n",
    "Xte             = use[4]\n",
    "yte             = use[5]\n",
    "Xtr_val         = use[6]\n",
    "ytr_val         = use[7]\n",
    "Xte_merged      = use[8]\n",
    "yte_merged      = use[9]\n",
    "id_tr           = use[10]\n",
    "id_v            = use[11]\n",
    "id_te           = use[12]\n",
    "window_train    = use[13]\n",
    "window_val      = use[14]\n",
    "window_test     = use[15]\n",
    "X_scaler        = use[16]\n",
    "y_scaler        = use[17]\n",
    "X_scaler_merged = use[18]\n",
    "y_scaler_merged = use[19]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f86f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 2**16\n",
    "batch = (Xtr[:num_data], ytr[:num_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f73b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def clean_gpu():\n",
    "    # 1. Delete the model variable if it exists\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "    \n",
    "    # 2. Clear Hidden References (The \"Jupyter Trap\")\n",
    "    # Jupyter stores the last error in these variables. \n",
    "    # If your OOM error holds the stack frame, it holds the model.\n",
    "    if hasattr(sys, 'last_traceback'):\n",
    "        del sys.last_traceback\n",
    "    if hasattr(sys, 'last_value'):\n",
    "        del sys.last_value\n",
    "    if hasattr(sys, 'last_type'):\n",
    "        del sys.last_type\n",
    "\n",
    "    # 3. Clear Loop Variables\n",
    "    # If you crashed inside a loop, 'b' or 'batch' might still hold refs\n",
    "    if 'inputs' in globals(): del globals()['inputs']\n",
    "    if 'targets' in globals(): del globals()['targets']\n",
    "    if 'grads' in globals(): del globals()['grads']\n",
    "\n",
    "    # 4. Force Garbage Collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # 5. Clear PyTorch Cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "clean_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485edaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_optim(path):\n",
    "    # 1. Isolate the directory name starting with 'exp_'\n",
    "    # We strip '/' to handle trailing slashes safely\n",
    "    parts = path.strip('/').split('/')\n",
    "    exp_dir = next((p for p in parts if p.startswith('exp_')), None)\n",
    "    \n",
    "    if not exp_dir:\n",
    "        return None\n",
    "    \n",
    "    # 2. Split the experiment name by underscore\n",
    "    # Structure: exp_{id}_{arch}_{size}_{optim}_{extra...}\n",
    "    # Indices:    0    1     2      3      4\n",
    "    tokens = exp_dir.split('_')\n",
    "    \n",
    "    # 3. Combine Arch (2) and Optim (4)\n",
    "    if len(tokens) >= 5:\n",
    "        return f\"{tokens[2]}_{tokens[4]}\"\n",
    "    return None\n",
    "\n",
    "# --- usage ---\n",
    "# Output: 'transformer_sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5da94319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_muon | Lambda Max: 19419.482421875\n",
      "cnn_adam | Lambda Max: 358.6480407714844\n",
      "cnn_sgd | Lambda Max: 76.18799591064453\n",
      "mlp_muon | Lambda Max: 158.217529296875\n",
      "mlp_adam | Lambda Max: 166.74081420898438\n",
      "mlp_sgd | Lambda Max: 121.89754486083984\n"
     ]
    }
   ],
   "source": [
    "base_dirs = [\n",
    " '/orcd/home/002/corte911/code/finNN_code/src/volatility/experiments/exp_169_cnn_100_muon_icml_3/trial_search_best/',\n",
    " '/orcd/home/002/corte911/code/finNN_code/src/volatility/experiments/exp_037_cnn_100_adam_lr/trial_search_best/',\n",
    " '/orcd/home/002/corte911/code/finNN_code/src/volatility/experiments/exp_041_cnn_100_sgd/trial_search_best/',\n",
    " '/orcd/home/002/corte911/code/finNN_code/src/volatility/experiments/exp_035_mlp_100_muon_lr/trial_search_best/',\n",
    " '/orcd/home/002/corte911/code/finNN_code/src/volatility/experiments/exp_038_mlp_100_adam_lr/trial_search_best/',\n",
    " '/orcd/home/002/corte911/code/finNN_code/src/volatility/experiments/exp_043_mlp_100_sgd/trial_search_best/']\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for b in base_dirs:\n",
    "    # Load\n",
    "    model = load_nn(b, batch[0])\n",
    "    if model is None: continue\n",
    "        \n",
    "    try:\n",
    "        # Run optimized estimation\n",
    "        # Note: We pass batch_size=32 explicitly here\n",
    "        l_max = estimate_lambda_max_optimized(\n",
    "            model, \n",
    "            criterion, \n",
    "            batch, \n",
    "            n_iter=50, \n",
    "            batch_size=32 # Reduces VRAM usage drastically\n",
    "        )\n",
    "        print(f\"{extract_model_optim(b)} | Lambda Max: {l_max}\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Skipping {b} due to error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Delete model and clear cache between runs\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared. Allocated: 115.05 GB\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_h200_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
