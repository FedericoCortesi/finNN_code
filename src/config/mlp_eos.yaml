model:
  name: mlp
  hparams:
    mlp_hidden_sizes:
      - 512
      - 256
      - 256
      - 128
    mlp_activation:
      - relu
      - relu
      - relu
      - relu
    dropout_rate: 0

    # (kept even if unused by MLP, since present in your config)
    lstm_hidden_sizes:
      - 256
      - 256
    lstm_dropout: 0
    bidirectional: false
    readout: last
    use_ln: false

    conv_channels:
      - 64
      - 128
      - 256
    conv_activation:
      - relu
      - relu
      - relu
    kernel_size: 3
    padding: 1
    pool: adaptive_max
    pool_k: 4
    use_bn: false

    output_activation: linear
    n_layers: 2
data: 
  df_path:  # if left out the pipeline takes it automatically
  df_master: # if left out the pipeline takes it automatically


trainer:
  hparams:
    epochs: 50
    batch_size: 512
    torch_patience: 25
    min_delta: 1e-10
    optuna_patience: 1000
    loss: mse
    metrics:
      - mae
      - mse
      - dir_acc
      - qlike
    val_every: 1
    optimizer_type: sgd
    weight_decay: 0.0018197007920120467
  search:
    lr:
      type: cat 
      choices: [0.025, 0.075, 0.0075] 

walkforward:
  target_col: var
  lookback: 0
  ratio_train: 3
  ratio_val: 1
  ratio_test: 1
  step: 1257
  lags: 100
  max_folds: null
  min_folds: null
  scale: true
  annualize: true
  scale_type: logstandard
  clip: 0.1

experiment:
  name: eos_mlp_v2
  hyperparams_search: true
  monitor: val_loss
  mode: min
  type: volatility
  n_trials: 5
  random_state: 42
