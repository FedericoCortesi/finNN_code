experiment:
  name: "debug"
  type: "price_prediction"
  random_state: 1234
  monitor: "val_loss"
  mode: "min"
  n_trials: 10 # for random search 
  random_state: 42 # for reproducibility
  hyperparams_search: False

# this whole thing might be useless
data: 
  # df_path: "data/sp500_daily_data.parquet" # if left out the pipeline takes it automatically
  df_master:  # if left out the pipeline takes it automatically

walkforward:
  target_col: ret
  lookback: 5
  max_folds: 
  step: 251
  ratio_train: 3
  ratio_val: 1
  ratio_test: 1
  lags: 40
  scale: True
  clip: False
  #purge_days: 20
  #embargo_days: 20

trainer:
  hparams:
    epochs: 50
    batch_size: 512 # P. said 256-512 to "kill some noise"
    torch_patience:  # if empty early stopping is not active
    min_delta: 1e-10 # very small when not startdadized 
    optuna_patience: 25  
    loss: "mse"
    metrics: ["mae","mse", "dir_acc", "qlike"]
    val_every: 1
    lr: 1e-3
    weight_decay:  0 # change to zero


model:
  name: "lstm"
  hparams:  
    # ==========================
    #  MLP HEAD PARAMETERS
    # ==========================
    mlp_hidden_sizes: [1024]      # Fully connected layers after LSTM readout
    mlp_activation: [relu]    # Activations for each MLP layer
    dropout_rate: 0                             # Dropout applied in MLP layers

    # ==========================
    #  LSTM CORE PARAMETERS
    # ==========================
    lstm_hidden_sizes: [1024]     # Hidden size(s) of the LSTM (stack depth = len(list))
    lstm_dropout: 0                             # Dropout between stacked LSTM layers
    bidirectional: False                        # If True, concatenates forward & backward hidden states
    readout: last                               # "last" | "mean" | "max"  â†’ how to summarize sequence output
    use_ln: False                               # Apply LayerNorm on the readout (stabilizes noisy sequences)

    # ==========================
    #  CNN BLOCK PARAMETERS
    # ==========================
    conv_channels: [64, 128, 256]               # Channels per Conv1D block
    conv_activation: [relu, relu, relu]         # Activations for each Conv1D block
    kernel_size: 3                              # Convolution kernel size
    padding: 1                                  # Convolution padding
    pool: adaptive_max                          # "adaptive_avg" or "adaptive_max"
    pool_k: 4                                   # Output size of pooling layer
    use_bn: False                               # Apply BatchNorm in convolutional layers

    # ==========================
    #  OUTPUT PARAMETERS
    # ==========================
    output_activation: linear                   # Activation for final layer (e.g., linear for regression)

 

