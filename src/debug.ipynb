{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55393ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b1073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from config.config_types import AppConfig\n",
    "from utils.logging_utils import ExperimentLogger\n",
    "from training_routine.trainer import Trainer            \n",
    "from pipeline.walkforward import WFCVGenerator\n",
    "from pipeline.preprocessing import preprocess\n",
    "from hyperparams_search.search_utils import sample_hparams_into_cfg\n",
    "#from hyperparams_search.torch_estimator import TorchFoldEstimator\n",
    "#from hyperparams_search.randomsearch import RandomSearch\n",
    "\n",
    "from utils.gpu_test import gpu_test\n",
    "from utils.paths import CONFIG_DIR\n",
    "from utils.custom_formatter import setup_logger\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from utils.logging_utils import ExperimentLogger\n",
    "from training_routine.trainer import Trainer            \n",
    "from pipeline.walkforward import WFCVGenerator\n",
    "from utils.gpu_test import gpu_test\n",
    "from utils.paths import CONFIG_DIR, SP500COPY_PATH, SP500_PATH\n",
    "from utils.custom_formatter import setup_logger\n",
    "\n",
    "from models import create_model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b6354",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d9ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AppConfig.from_dict(f\"{CONFIG_DIR}/debug.yaml\")\n",
    "\n",
    "preprocess(path=SP500_PATH)\n",
    "\n",
    "\n",
    "wf = WFCVGenerator(config=cfg.walkforward)\n",
    "\n",
    "df = wf.df_master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f027f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f8fb88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m13:50:06 - Experiment - INFO - GPU check complete. (3647976701.py:6)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AppConfig(model=ModelConfig(name='mlp', hparams={'hidden_sizes': [512, 256, 256, 128], 'dropout_rate': 0, 'activation': ['relu', 'relu', 'relu', 'relu'], 'output_activation': 'linear', 'conv_channels': [32], 'use_bn': False, 'kernel_size': 3, 'padding': 3, 'pool': 'adaptive_avg', 'pool_k': 1}, search={}), trainer=TrainerConfig(hparams={'epochs': 250, 'batch_size': 512, 'lr': '5e-5', 'loss': 'mse', 'metrics': ['mae', 'mse', 'dir_acc'], 'weight_decay': 0, 'val_every': 1}, search={}), walkforward=WFConfig(target_col='var', lookback=5, ratio_train=3, ratio_val=1, ratio_test=1, step=251, lags=40, max_folds=None, scale=True), experiment=ExperimentConfig(name='mlp_40_sliding', hyperparams_search=False, monitor='val_loss', mode='min', type='volatility', n_trials=20, random_state=1234), data={'df_master': None})\n",
      "WFConfig(target_col='var', lookback=5, ratio_train=3, ratio_val=1, ratio_test=1, step=251, lags=40, max_folds=None, scale=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setup logger\n",
    "console_logger = setup_logger(\"Experiment\", level=\"INFO\")\n",
    "\n",
    "# --- GPU check (PyTorch) ---\n",
    "gpu_test()\n",
    "console_logger.info(\"GPU check complete.\")\n",
    "\n",
    "# -------- load config --------\n",
    "cfg = AppConfig.from_dict(f\"{CONFIG_DIR}/debug.yaml\")\n",
    "print(cfg)\n",
    "\n",
    "print(cfg.walkforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac288db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------- data + components --------\n",
    "#logger = ExperimentLogger(cfg)\n",
    "\n",
    "\n",
    "wf = WFCVGenerator(config=cfg.walkforward)\n",
    "\n",
    "for i, fold_data in enumerate(wf.folds()):\n",
    "    if i > 0:\n",
    "        break\n",
    "\n",
    "    X_test = fold_data[4]\n",
    "    y_test = fold_data[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77516b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;20m17:32:52 - Experiment - WARNING - No hyperparams search for this experiment (3305620216.py:30)\u001b[0m\n",
      "\u001b[33;20m17:32:52 - ExperimentLogger - WARNING - Experiment with name 'MLP_linreg' already exists: /orcd/home/002/corte911/code/finNN_code/src/price_prediction/experiments/exp_008_MLP_linreg. If the experiment is of type `search`, 'trial_serch_best' will be overwritten!Creating a new trial under this experiment. (logging_utils.py:64)\u001b[0m\n",
      "\u001b[38;20m17:32:52 - ExperimentLogger - INFO - Experiment directory: /orcd/home/002/corte911/code/finNN_code/src/price_prediction/experiments/exp_008_MLP_linreg (logging_utils.py:92)\u001b[0m\n",
      "\u001b[38;20m17:32:52 - ExperimentLogger - INFO - Trial directory: /orcd/home/002/corte911/code/finNN_code/src/price_prediction/experiments/exp_008_MLP_linreg/trial_20251019_173252 (logging_utils.py:135)\u001b[0m\n",
      "\u001b[38;20m17:32:55 - Trainer - INFO - Fitting fold 00 (trial 00) on torch.Size([364349, 21]) samples, val torch.Size([115317, 21]), test torch.Size([115175, 21]) (trainer.py:180)\u001b[0m\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0] failed while attempting to run meta for aten.mm.default\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0] Traceback (most recent call last):\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]   File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py\", line 2013, in _dispatch_impl\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]     r = func(*args, **kwargs)\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]   File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_ops.py\", line 716, in __call__\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]     return self._op(*args, **kwargs)\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]   File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_prims_common/wrappers.py\", line 273, in _fn\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]     result = fn(*args, **kwargs)\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]              ^^^^^^^^^^^^^^^^^^^\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]   File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_meta_registrations.py\", line 2100, in meta_mm\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]     torch._check(\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]   File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/__init__.py\", line 1564, in _check\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]     _check_with(RuntimeError, cond, message)\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]   File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/__init__.py\", line 1546, in _check_with\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0]     raise error_type(message_evaluated)\n",
      "E1019 17:32:56.087000 3657465 /orcd/home/002/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2017] [0/0] RuntimeError: a and b must have same reduction dim, but got [512, 21] X [20, 1024].\n"
     ]
    },
    {
     "ename": "TorchRuntimeError",
     "evalue": "Failed running call_function <built-in function linear>(*(FakeTensor(..., device='cuda:0', size=(512, 21)), Parameter(FakeTensor(..., device='cuda:0', size=(1024, 20), requires_grad=True)), Parameter(FakeTensor(..., device='cuda:0', size=(1024,), requires_grad=True))), **{}):\na and b must have same reduction dim, but got [512, 21] X [20, 1024].\n\nfrom user code:\n   File \"/orcd/home/002/corte911/code/finNN_code/src/models/mlp.py\", line 82, in forward\n    return self.net(x)\n  File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n  File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTorchRuntimeError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fold == \u001b[32m0\u001b[39m:\n\u001b[32m     45\u001b[39m             console_logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_eval_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m console_logger.warning(\u001b[33m\"\u001b[39m\u001b[33mTraining completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/corte911/code/finNN_code/src/training_routine/trainer.py:195\u001b[39m, in \u001b[36mTrainer.fit_eval_fold\u001b[39m\u001b[34m(self, model, data, fold, trial, merge_train_val, report_cb)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m amp_ctx:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m.squeeze(-\u001b[32m1\u001b[39m)\n\u001b[32m    196\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.loss_fn(pred, yb)\n\u001b[32m    197\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:465\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m saved_dynamic_layer_stack_depth = (\n\u001b[32m    461\u001b[39m     torch._C._functorch.get_dynamic_layer_stack_depth()\n\u001b[32m    462\u001b[39m )\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    468\u001b[39m     torch._C._functorch.pop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[32m    469\u001b[39m         saved_dynamic_layer_stack_depth\n\u001b[32m    470\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[39m, in \u001b[36mCatchErrorsWrapper.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, frame_state)\u001b[39m\n\u001b[32m   1263\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[32m   1264\u001b[39m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m.hooks, frame_state\n\u001b[32m   1265\u001b[39m             )\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:1064\u001b[39m, in \u001b[36mConvertFrame.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m   1062\u001b[39m counters[\u001b[33m\"\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m     counters[\u001b[33m\"\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:526\u001b[39m, in \u001b[36mConvertFrameAssert.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m    510\u001b[39m compile_id = CompileId(frame_id, frame_compile_id)\n\u001b[32m    512\u001b[39m signpost_event(\n\u001b[32m    513\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdynamo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    514\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m_convert_frame_assert._compile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m     },\n\u001b[32m    524\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:924\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[39m\n\u001b[32m    922\u001b[39m guarded_code = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m     guarded_code = \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:666\u001b[39m, in \u001b[36m_compile.<locals>.compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33m_compile.compile_inner\u001b[39m\u001b[33m\"\u001b[39m, phase_name=\u001b[33m\"\u001b[39m\u001b[33mentire_frame_compile\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    665\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter.record():\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_utils_internal.py:87\u001b[39m, in \u001b[36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m] = kwargs[\u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler.enabled:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler.profile_compile_time(\n\u001b[32m     90\u001b[39m     function, phase_name, *args, **kwargs\n\u001b[32m     91\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:699\u001b[39m, in \u001b[36m_compile.<locals>._compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    697\u001b[39m CompileContext.get().attempt = attempt\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     out_code = \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.RestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[39m, in \u001b[36mtransform_code_object\u001b[39m\u001b[34m(code, transformations, safe)\u001b[39m\n\u001b[32m   1319\u001b[39m instructions = cleaned_instructions(code, safe)\n\u001b[32m   1320\u001b[39m propagate_line_nums(instructions)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:219\u001b[39m, in \u001b[36mpreserve_global_state.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m exit_stack.enter_context(\n\u001b[32m    216\u001b[39m     torch.fx._symbolic_trace._maybe_revert_all_patches()\n\u001b[32m    217\u001b[39m )\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     cleanup.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py:634\u001b[39m, in \u001b[36m_compile.<locals>.transform\u001b[39m\u001b[34m(instructions, code_options)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    633\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer.output.tracing_context), tracer.set_current_tx():\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m         \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.UnspecializeRestartAnalysis:\n\u001b[32m    636\u001b[39m     speculation_log.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[39m, in \u001b[36mInstructionTranslator.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2795\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2796\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[39m, in \u001b[36mInstructionTranslatorBase.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     \u001b[38;5;28mself\u001b[39m.output.push_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    984\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[39m, in \u001b[36mInstructionTranslatorBase.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;28mself\u001b[39m.update_block_stack(inst)\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output.should_exit\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.ObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[39m, in \u001b[36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation.reason)\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generic_context_manager_depth > \u001b[32m0\u001b[39m:\n\u001b[32m    585\u001b[39m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[32m    586\u001b[39m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2279\u001b[39m, in \u001b[36mInstructionTranslatorBase.CALL\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m   2277\u001b[39m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push=\u001b[32m1\u001b[39m)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2273\u001b[39m, in \u001b[36mInstructionTranslatorBase._call\u001b[39m\u001b[34m(self, inst, call_kw)\u001b[39m\n\u001b[32m   2268\u001b[39m     kwargs = {}\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2271\u001b[39m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[32m   2272\u001b[39m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2273\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2274\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2275\u001b[39m     \u001b[38;5;28mself\u001b[39m.kw_names = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[39m, in \u001b[36mInstructionTranslatorBase.call_function\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28mself\u001b[39m.push(\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/lazy.py:156\u001b[39m, in \u001b[36m_create_realize_and_forward.<locals>.realize_and_forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(\u001b[38;5;28mgetattr\u001b[39m(VariableTracker, name))\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrealize_and_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrealize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/nn_module.py:899\u001b[39m, in \u001b[36mUnspecializedNNModuleVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m    891\u001b[39m ctx = (\n\u001b[32m    892\u001b[39m     record_nn_module_stack(\n\u001b[32m    893\u001b[39m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(mod)), \u001b[38;5;28mself\u001b[39m.get_nn_module_stack_source(), tx, mod\n\u001b[32m   (...)\u001b[39m\u001b[32m    896\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[32m    897\u001b[39m )\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvariables\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:324\u001b[39m, in \u001b[36mUserFunctionVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_constant:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[32m    321\u001b[39m         tx, \u001b[38;5;28mself\u001b[39m.fn, \u001b[38;5;28mself\u001b[39m.get_name(), args, kwargs\n\u001b[32m    322\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:111\u001b[39m, in \u001b[36mBaseUserFunctionVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_function\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    107\u001b[39m     tx: \u001b[33m\"\u001b[39m\u001b[33mInstructionTranslator\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    108\u001b[39m     args: \u001b[33m\"\u001b[39m\u001b[33mList[VariableTracker]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m     kwargs: \u001b[33m\"\u001b[39m\u001b[33mDict[str, VariableTracker]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    110\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mVariableTracker\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:836\u001b[39m, in \u001b[36mInstructionTranslatorBase.inline_user_function_return\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m    833\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[33;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:3011\u001b[39m, in \u001b[36mInliningInstructionTranslator.inline_call\u001b[39m\u001b[34m(cls, parent, func, args, kwargs)\u001b[39m\n\u001b[32m   3008\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   3009\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[32m   3010\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m patch.dict(counters, {\u001b[33m\"\u001b[39m\u001b[33munimplemented\u001b[39m\u001b[33m\"\u001b[39m: counters[\u001b[33m\"\u001b[39m\u001b[33minline_call\u001b[39m\u001b[33m\"\u001b[39m]}):\n\u001b[32m-> \u001b[39m\u001b[32m3011\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:3139\u001b[39m, in \u001b[36mInliningInstructionTranslator.inline_call_\u001b[39m\u001b[34m(parent, func, args, kwargs)\u001b[39m\n\u001b[32m   3137\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3138\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[32m-> \u001b[39m\u001b[32m3139\u001b[39m         \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.ObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3141\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[39m, in \u001b[36mInstructionTranslatorBase.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     \u001b[38;5;28mself\u001b[39m.output.push_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    984\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[39m, in \u001b[36mInstructionTranslatorBase.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;28mself\u001b[39m.update_block_stack(inst)\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output.should_exit\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.ObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[39m, in \u001b[36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation.reason)\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generic_context_manager_depth > \u001b[32m0\u001b[39m:\n\u001b[32m    585\u001b[39m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[32m    586\u001b[39m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2279\u001b[39m, in \u001b[36mInstructionTranslatorBase.CALL\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m   2277\u001b[39m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push=\u001b[32m1\u001b[39m)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2273\u001b[39m, in \u001b[36mInstructionTranslatorBase._call\u001b[39m\u001b[34m(self, inst, call_kw)\u001b[39m\n\u001b[32m   2268\u001b[39m     kwargs = {}\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2271\u001b[39m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[32m   2272\u001b[39m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2273\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2274\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2275\u001b[39m     \u001b[38;5;28mself\u001b[39m.kw_names = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[39m, in \u001b[36mInstructionTranslatorBase.call_function\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28mself\u001b[39m.push(\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/nn_module.py:899\u001b[39m, in \u001b[36mUnspecializedNNModuleVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m    891\u001b[39m ctx = (\n\u001b[32m    892\u001b[39m     record_nn_module_stack(\n\u001b[32m    893\u001b[39m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(mod)), \u001b[38;5;28mself\u001b[39m.get_nn_module_stack_source(), tx, mod\n\u001b[32m   (...)\u001b[39m\u001b[32m    896\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[32m    897\u001b[39m )\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m--> \u001b[39m\u001b[32m899\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvariables\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUserFunctionVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:324\u001b[39m, in \u001b[36mUserFunctionVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_constant:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[32m    321\u001b[39m         tx, \u001b[38;5;28mself\u001b[39m.fn, \u001b[38;5;28mself\u001b[39m.get_name(), args, kwargs\n\u001b[32m    322\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:111\u001b[39m, in \u001b[36mBaseUserFunctionVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_function\u001b[39m(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    107\u001b[39m     tx: \u001b[33m\"\u001b[39m\u001b[33mInstructionTranslator\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    108\u001b[39m     args: \u001b[33m\"\u001b[39m\u001b[33mList[VariableTracker]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m     kwargs: \u001b[33m\"\u001b[39m\u001b[33mDict[str, VariableTracker]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    110\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mVariableTracker\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:836\u001b[39m, in \u001b[36mInstructionTranslatorBase.inline_user_function_return\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m    833\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[33;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[43m.\u001b[49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:3011\u001b[39m, in \u001b[36mInliningInstructionTranslator.inline_call\u001b[39m\u001b[34m(cls, parent, func, args, kwargs)\u001b[39m\n\u001b[32m   3008\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   3009\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[32m   3010\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m patch.dict(counters, {\u001b[33m\"\u001b[39m\u001b[33munimplemented\u001b[39m\u001b[33m\"\u001b[39m: counters[\u001b[33m\"\u001b[39m\u001b[33minline_call\u001b[39m\u001b[33m\"\u001b[39m]}):\n\u001b[32m-> \u001b[39m\u001b[32m3011\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:3139\u001b[39m, in \u001b[36mInliningInstructionTranslator.inline_call_\u001b[39m\u001b[34m(parent, func, args, kwargs)\u001b[39m\n\u001b[32m   3137\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3138\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[32m-> \u001b[39m\u001b[32m3139\u001b[39m         \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.ObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3141\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[39m, in \u001b[36mInstructionTranslatorBase.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     \u001b[38;5;28mself\u001b[39m.output.push_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    984\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[39m, in \u001b[36mInstructionTranslatorBase.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;28mself\u001b[39m.update_block_stack(inst)\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output.should_exit\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.ObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[39m, in \u001b[36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation.reason)\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generic_context_manager_depth > \u001b[32m0\u001b[39m:\n\u001b[32m    585\u001b[39m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[32m    586\u001b[39m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2279\u001b[39m, in \u001b[36mInstructionTranslatorBase.CALL\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m   2277\u001b[39m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push=\u001b[32m1\u001b[39m)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:2273\u001b[39m, in \u001b[36mInstructionTranslatorBase._call\u001b[39m\u001b[34m(self, inst, call_kw)\u001b[39m\n\u001b[32m   2268\u001b[39m     kwargs = {}\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2271\u001b[39m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[32m   2272\u001b[39m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2273\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2274\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2275\u001b[39m     \u001b[38;5;28mself\u001b[39m.kw_names = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[39m, in \u001b[36mInstructionTranslatorBase.call_function\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28mself\u001b[39m.push(\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/torch.py:897\u001b[39m, in \u001b[36mTorchInGraphFunctionVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m    888\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mout\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mout\u001b[39m\u001b[33m\"\u001b[39m], variables.TensorVariable):\n\u001b[32m    889\u001b[39m                 \u001b[38;5;66;03m# Calling fake tensor propagation can mutate the out= tensor in\u001b[39;00m\n\u001b[32m    890\u001b[39m                 \u001b[38;5;66;03m# tx.output.tracked_fakes. tracked_fakes are used to apply\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    893\u001b[39m                 \u001b[38;5;66;03m# guards. So save the shape now, and check later if it has\u001b[39;00m\n\u001b[32m    894\u001b[39m                 \u001b[38;5;66;03m# changed. If it has, graph break.\u001b[39;00m\n\u001b[32m    895\u001b[39m                 fake_out_shape = kwargs[\u001b[33m\"\u001b[39m\u001b[33mout\u001b[39m\u001b[33m\"\u001b[39m].proxy.node.meta[\u001b[33m\"\u001b[39m\u001b[33mexample_value\u001b[39m\u001b[33m\"\u001b[39m].shape\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m             tensor_variable = \u001b[43mwrap_fx_proxy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m                \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_proxy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcall_function\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfn_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mproxy_args_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    906\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    907\u001b[39m                 \u001b[38;5;28misinstance\u001b[39m(tensor_variable, TensorVariable)\n\u001b[32m    908\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrequires_grad\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[32m    909\u001b[39m                 \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mrequires_grad\u001b[39m\u001b[33m\"\u001b[39m].as_python_constant()\n\u001b[32m    910\u001b[39m             ):\n\u001b[32m    911\u001b[39m                 unimplemented(\n\u001b[32m    912\u001b[39m \u001b[38;5;250m                    \u001b[39m\u001b[33;03m\"\"\"factory functions that return tensors that require grad are not supported.\u001b[39;00m\n\u001b[32m    913\u001b[39m \u001b[33;03mEither create the tensor outside the compiled region, or do not set the tensor to require_grad\"\"\"\u001b[39;00m\n\u001b[32m    914\u001b[39m                 )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py:2037\u001b[39m, in \u001b[36mwrap_fx_proxy\u001b[39m\u001b[34m(tx, proxy, example_value, subclass_type, **options)\u001b[39m\n\u001b[32m   2029\u001b[39m kwargs = {\n\u001b[32m   2030\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtx\u001b[39m\u001b[33m\"\u001b[39m: tx,\n\u001b[32m   2031\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproxy\u001b[39m\u001b[33m\"\u001b[39m: proxy,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2034\u001b[39m     **options,\n\u001b[32m   2035\u001b[39m }\n\u001b[32m   2036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subclass_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTensorVariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2039\u001b[39m     result = wrap_fx_proxy_cls(target_cls=TensorWithTFOverrideVariable, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py:2124\u001b[39m, in \u001b[36mwrap_fx_proxy_cls\u001b[39m\u001b[34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[39m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch._dynamo.utils._disable_saved_tensors_hooks_during_tracing():\n\u001b[32m   2120\u001b[39m     \u001b[38;5;66;03m# with preserve_rng_state():\u001b[39;00m\n\u001b[32m   2121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m example_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2122\u001b[39m         \u001b[38;5;66;03m# only allow_non_graph_fake in this instance because we handle the non-fake\u001b[39;00m\n\u001b[32m   2123\u001b[39m         \u001b[38;5;66;03m# cases properly below.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2124\u001b[39m         example_value = \u001b[43mget_fake_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_graph_fake\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2126\u001b[39m     \u001b[38;5;66;03m# Handle recursive calls here\u001b[39;00m\n\u001b[32m   2127\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m maybe_get_fake_mode(example_value) \u001b[38;5;129;01mis\u001b[39;00m tx.fake_mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/utils.py:2082\u001b[39m, in \u001b[36mget_fake_value\u001b[39m\u001b[34m(node, tx, allow_non_graph_fake)\u001b[39m\n\u001b[32m   2079\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cause, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33margument\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(cause):\n\u001b[32m   2080\u001b[39m         unimplemented(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTypeError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode.target\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcause\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2082\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TorchRuntimeError(\u001b[38;5;28mstr\u001b[39m(e)).with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2084\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_non_graph_fake:\n\u001b[32m   2085\u001b[39m     _ = pytree.tree_map_only(\n\u001b[32m   2086\u001b[39m         torch.Tensor, functools.partial(ensure_graph_fake, tx=tx), ret_val\n\u001b[32m   2087\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/utils.py:2017\u001b[39m, in \u001b[36mget_fake_value\u001b[39m\u001b[34m(node, tx, allow_non_graph_fake)\u001b[39m\n\u001b[32m   2015\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2016\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tx.fake_mode, enable_python_dispatcher():\n\u001b[32m-> \u001b[39m\u001b[32m2017\u001b[39m         ret_val = \u001b[43mwrap_fake_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[32m   2021\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/utils.py:1574\u001b[39m, in \u001b[36mwrap_fake_exception\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m   1572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_fake_exception\u001b[39m(fn):\n\u001b[32m   1573\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1574\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1575\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1576\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unimplemented\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/utils.py:2018\u001b[39m, in \u001b[36mget_fake_value.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2015\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2016\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tx.fake_mode, enable_python_dispatcher():\n\u001b[32m   2017\u001b[39m         ret_val = wrap_fake_exception(\n\u001b[32m-> \u001b[39m\u001b[32m2018\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2019\u001b[39m         )\n\u001b[32m   2020\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[32m   2021\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/utils.py:2150\u001b[39m, in \u001b[36mrun_node\u001b[39m\u001b[34m(tracer, node, args, kwargs, nnmodule)\u001b[39m\n\u001b[32m   2148\u001b[39m         unimplemented(make_error_message(e), from_exc=e)\n\u001b[32m   2149\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(make_error_message(e)).with_traceback(\n\u001b[32m   2151\u001b[39m             e.__traceback__\n\u001b[32m   2152\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2154\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(op)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_dynamo/utils.py:2132\u001b[39m, in \u001b[36mrun_node\u001b[39m\u001b[34m(tracer, node, args, kwargs, nnmodule)\u001b[39m\n\u001b[32m   2130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op == \u001b[33m\"\u001b[39m\u001b[33mcall_function\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2133\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m op == \u001b[33m\"\u001b[39m\u001b[33mcall_method\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(args[\u001b[32m0\u001b[39m], node.target)(*args[\u001b[32m1\u001b[39m:], **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/utils/_stats.py:21\u001b[39m, in \u001b[36mcount.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m     simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m     20\u001b[39m simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1238\u001b[39m, in \u001b[36mFakeTensorMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m   1235\u001b[39m     torch._C._get_dispatch_mode(torch._C._TorchDispatchModeKey.FAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1236\u001b[39m ), func\n\u001b[32m   1237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1240\u001b[39m     log.exception(\u001b[33m\"\u001b[39m\u001b[33mfake tensor raised TypeError\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1692\u001b[39m, in \u001b[36mFakeTensorMode.dispatch\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1689\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_enabled:\n\u001b[32m-> \u001b[39m\u001b[32m1692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1694\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dispatch_impl(func, types, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1339\u001b[39m, in \u001b[36mFakeTensorMode._cached_dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_cache_key(func, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1339\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m     entry = \u001b[38;5;28mself\u001b[39m._make_cache_entry(state, key, func, args, kwargs, output)\n\u001b[32m   1341\u001b[39m     key.strip_shape_env()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1943\u001b[39m, in \u001b[36mFakeTensorMode._dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1933\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m decomposition_table \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1934\u001b[39m     has_symbolic_sizes\n\u001b[32m   1935\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1940\u001b[39m     )\n\u001b[32m   1941\u001b[39m ):\n\u001b[32m   1942\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1943\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecomposition_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1946\u001b[39m     \u001b[38;5;66;03m# Decomposes CompositeImplicitAutograd ops\u001b[39;00m\n\u001b[32m   1947\u001b[39m     r = func.decompose(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_prims_common/wrappers.py:273\u001b[39m, in \u001b[36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[39m\u001b[34m(out, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     result = fn(*args, is_out=(out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), **kwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    275\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(result, TensorLike)\n\u001b[32m    276\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_tensor\n\u001b[32m    277\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Tuple)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    278\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) == \u001b[38;5;28mlen\u001b[39m(out_names)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    279\u001b[39m )\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# it's not:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m# harmless.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_decomp/decompositions.py:83\u001b[39m, in \u001b[36mtype_casts.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m r = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincrease_prec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincrease_prec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compute_dtype_only:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_decomp/decompositions.py:1515\u001b[39m, in \u001b[36maddmm\u001b[39m\u001b[34m(self, mat1, mat2, beta, alpha)\u001b[39m\n\u001b[32m   1513\u001b[39m     beta = \u001b[38;5;28mint\u001b[39m(beta)\n\u001b[32m   1514\u001b[39m     alpha = \u001b[38;5;28mint\u001b[39m(alpha)\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m out = alpha * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m beta == \u001b[32m0\u001b[39m:\n\u001b[32m   1517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/utils/_stats.py:21\u001b[39m, in \u001b[36mcount.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m     simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m     20\u001b[39m simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1238\u001b[39m, in \u001b[36mFakeTensorMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m   1235\u001b[39m     torch._C._get_dispatch_mode(torch._C._TorchDispatchModeKey.FAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1236\u001b[39m ), func\n\u001b[32m   1237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1240\u001b[39m     log.exception(\u001b[33m\"\u001b[39m\u001b[33mfake tensor raised TypeError\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1692\u001b[39m, in \u001b[36mFakeTensorMode.dispatch\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1689\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_enabled:\n\u001b[32m-> \u001b[39m\u001b[32m1692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1694\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dispatch_impl(func, types, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:1339\u001b[39m, in \u001b[36mFakeTensorMode._cached_dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_cache_key(func, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1339\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m     entry = \u001b[38;5;28mself\u001b[39m._make_cache_entry(state, key, func, args, kwargs, output)\n\u001b[32m   1341\u001b[39m     key.strip_shape_env()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py:2013\u001b[39m, in \u001b[36mFakeTensorMode._dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   2011\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2012\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m in_kernel_invocation_manager(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2013\u001b[39m         r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2014\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m not_implemented_error:\n\u001b[32m   2015\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m maybe_run_unsafe_fallback(not_implemented_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_ops.py:716\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_prims_common/wrappers.py:273\u001b[39m, in \u001b[36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[39m\u001b[34m(out, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     result = fn(*args, is_out=(out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), **kwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    275\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(result, TensorLike)\n\u001b[32m    276\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_tensor\n\u001b[32m    277\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Tuple)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    278\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) == \u001b[38;5;28mlen\u001b[39m(out_names)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    279\u001b[39m )\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# it's not:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m# harmless.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/_meta_registrations.py:2100\u001b[39m, in \u001b[36mmeta_mm\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m   2098\u001b[39m N, M1 = a.shape\n\u001b[32m   2099\u001b[39m M2, P = b.shape\n\u001b[32m-> \u001b[39m\u001b[32m2100\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mM1\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mM2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2102\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma and b must have same reduction dim, but got [\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mN\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mM1\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m] X [\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mM2\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mP\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m].\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a.new_empty(N, P)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/__init__.py:1564\u001b[39m, in \u001b[36m_check\u001b[39m\u001b[34m(cond, message)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check\u001b[39m(cond, message=\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   1550\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Throws error containing an optional message if the specified condition\u001b[39;00m\n\u001b[32m   1551\u001b[39m \u001b[33;03m    is False.\u001b[39;00m\n\u001b[32m   1552\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1562\u001b[39m \u001b[33;03m            message. Default: ``None``\u001b[39;00m\n\u001b[32m   1563\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1564\u001b[39m     \u001b[43m_check_with\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mRuntimeError\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/__init__.py:1546\u001b[39m, in \u001b[36m_check_with\u001b[39m\u001b[34m(error_type, cond, message)\u001b[39m\n\u001b[32m   1542\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmessage must be a callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1544\u001b[39m     message_evaluated = \u001b[38;5;28mstr\u001b[39m(message())\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error_type(message_evaluated)\n",
      "\u001b[31mTorchRuntimeError\u001b[39m: Failed running call_function <built-in function linear>(*(FakeTensor(..., device='cuda:0', size=(512, 21)), Parameter(FakeTensor(..., device='cuda:0', size=(1024, 20), requires_grad=True)), Parameter(FakeTensor(..., device='cuda:0', size=(1024,), requires_grad=True))), **{}):\na and b must have same reduction dim, but got [512, 21] X [20, 1024].\n\nfrom user code:\n   File \"/orcd/home/002/corte911/code/finNN_code/src/models/mlp.py\", line 82, in forward\n    return self.net(x)\n  File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n  File \"/home/corte911/.conda/envs/torch_h200_py312/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# instantiate trainer (PyTorch)\n",
    "#trainer = Trainer(cfg, logger)\n",
    "\n",
    "# model input size: number of lags (columns are constant across folds)\n",
    "input_shape = cfg.walkforward.lags            # int is fine; build_model handles it\n",
    "max_folds = cfg.walkforward.max_folds\n",
    "\n",
    "if cfg.model.name.lower() == \"cnn1d\":\n",
    "    input_shape = (1, cfg.walkforward.lags)  # (C, L)\n",
    "elif cfg.model.name.lower() == \"mlp\":\n",
    "    input_shape = (cfg.walkforward.lags,)\n",
    "else:\n",
    "    console_logger.warning(f\"Model: {cfg.model.name} not recognized!\")\n",
    "\n",
    "def make_input_shape(c):\n",
    "    if cfg.model.name.lower() == \"mlp\":\n",
    "        return (c.walkforward.lags, )\n",
    "    elif cfg.model.name.lower() == \"cnn1d\":\n",
    "        return (1, cfg.walkforward.lags)  # (C, L)\n",
    "    else:\n",
    "        console_logger.warning(f\"Model: {cfg.model.name} not recognized!\")\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "\n",
    "# Get bool for search\n",
    "hyperparams_search = cfg.experiment.hyperparams_search\n",
    "\n",
    "if not hyperparams_search:\n",
    "    console_logger.warning(\"No hyperparams search for this experiment\")\n",
    "    logger = ExperimentLogger(cfg) \n",
    "    trainer = Trainer(cfg, logger)\n",
    "    logger.begin_trial()\n",
    "\n",
    "# -------- train per fold --------\n",
    "for fold, data in enumerate(wf.folds()):\n",
    "    if max_folds is not None and fold >= max_folds:\n",
    "        break  # allow running subset of folds\n",
    "    else:\n",
    "        # Keep model creation inside the loop to avoid weight leakage across folds\n",
    "        # Keep model creation inside the loop to avoid weight leakage across folds\n",
    "        model = create_model(cfg.model, input_shape)       \n",
    "\n",
    "        if fold == 0:\n",
    "            console_logger.debug(f\"model: {model}\")\n",
    "\n",
    "        trainer.fit_eval_fold(model, data, trial=0, fold=fold)\n",
    "\n",
    "console_logger.warning(\"Training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bf3d3",
   "metadata": {},
   "source": [
    "test con giuseppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e495908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/home/002/corte911/code/finNN_code/src/pipeline/preprocessing.py:171: RuntimeWarning: invalid value encountered in log\n",
      "  log_co = np.log(close / open_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nans in var: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ret",
         "rawType": "Float64",
         "type": "float"
        },
        {
         "name": "var",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "4b288c76-3098-412e-9f07-d3ae6d086deb",
       "rows": [
        [
         "count",
         "3155297.0",
         "3155297.0"
        ],
        [
         "mean",
         "0.000476132277880656",
         "0.00047049180131714005"
        ],
        [
         "std",
         "0.024014246388803392",
         "0.05001886357421821"
        ],
        [
         "min",
         "-0.942466",
         "-0.01021524765722849"
        ],
        [
         "25%",
         "-0.009302",
         "7.993855360995712e-05"
        ],
        [
         "50%",
         "0.000435",
         "0.00016353330838816163"
        ],
        [
         "75%",
         "0.010181",
         "0.0003620660821196851"
        ],
        [
         "max",
         "1.023578",
         "84.23514653694176"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3155297.0</td>\n",
       "      <td>3.155297e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000476</td>\n",
       "      <td>4.704918e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.024014</td>\n",
       "      <td>5.001886e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.942466</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.009302</td>\n",
       "      <td>7.993855e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000435</td>\n",
       "      <td>1.635333e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.010181</td>\n",
       "      <td>3.620661e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.023578</td>\n",
       "      <td>8.423515e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ret           var\n",
       "count  3155297.0  3.155297e+06\n",
       "mean    0.000476  4.704918e-04\n",
       "std     0.024014  5.001886e-02\n",
       "min    -0.942466 -1.021525e-02\n",
       "25%    -0.009302  7.993855e-05\n",
       "50%     0.000435  1.635333e-04\n",
       "75%     0.010181  3.620661e-04\n",
       "max     1.023578  8.423515e+01"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess()\n",
    "df[[\"ret\", \"var\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba11f38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;20m17:16:27 - WFCVGenerator - DEBUG - WFConfig(train=753d, val=251d, test=251d, lags=20, step=251, max_folds=None) (walkforward.py:5)\u001b[0m\n",
      "\u001b[34;20m17:16:27 - WFCVGenerator - DEBUG - Loading data via preprocess() (walkforward.py:47)\u001b[0m\n",
      "/orcd/home/002/corte911/code/finNN_code/src/pipeline/preprocessing.py:171: RuntimeWarning: invalid value encountered in log\n",
      "  log_co = np.log(close / open_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nans in var: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;20m17:16:29 - WFCVGenerator - DEBUG - Loaded df: shape=(3155297, 3) dtypes={'t': dtype('int16'), 'var': dtype('float64'), 'permno': Int64Dtype()} head=\n",
      " t      var  permno\n",
      " 0 0.001527   10078\n",
      " 1 0.001098   10078\n",
      " 2 0.004587   10078 (walkforward.py:69)\u001b[0m\n",
      "\u001b[34;20m17:16:29 - WFCVGenerator - DEBUG - Predicting var (walkforward.py:12)\u001b[0m\n",
      "\u001b[34;20m17:16:29 - WFCVGenerator - DEBUG - self.df.columns: Index(['t', 'var', 'permno'], dtype='object') (walkforward.py:13)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cfg = AppConfig.from_dict(f\"{CONFIG_DIR}/debug.yaml\")\n",
    "wf = WFCVGenerator(config=cfg.walkforward)\n",
    "\n",
    "df = wf.df_master\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7638d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "feature_0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_13",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_16",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_17",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_18",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature_19",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "y",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cdb357e2-67ee-4fe9-90b6-4465f48faf54",
       "rows": [
        [
         "count",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0",
         "3133449.0"
        ],
        [
         "mean",
         "0.0004620279176569272",
         "0.00046184376557555713",
         "0.0004617616851852418",
         "0.00046163392092020997",
         "0.0004615435286833133",
         "0.0004615597880219473",
         "0.0004616128340638764",
         "0.0004616411762952811",
         "0.00046165594034565594",
         "0.000461721595722431",
         "0.00046175795068573194",
         "0.0004617576519497817",
         "0.00046179109785821933",
         "0.00046182133747423986",
         "0.000462111998513886",
         "0.0004621884447932605",
         "0.0004621985838332612",
         "0.00046636960028456387",
         "0.00046692766489560684",
         "0.0004675308249583682",
         "0.0004683461814116689"
        ],
        [
         "std",
         "0.04966727625353526",
         "0.04966726959441588",
         "0.04966727972286401",
         "0.04966727530411662",
         "0.04966728194267883",
         "0.049667330282710975",
         "0.0496673824355591",
         "0.04966742913179948",
         "0.04966745745692985",
         "0.0496675900663128",
         "0.04966762461781181",
         "0.049667673494207525",
         "0.04966768733038161",
         "0.0496677205046832",
         "0.049669144346230626",
         "0.04966939537736831",
         "0.04966957891296567",
         "0.05018398625622421",
         "0.05018788532359602",
         "0.050189693934452884",
         "0.05019274482735699"
        ],
        [
         "min",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849",
         "-0.01021524765722849"
        ],
        [
         "25%",
         "8.002705661950258e-05",
         "8.000649423989053e-05",
         "7.998366199533981e-05",
         "7.996174425235453e-05",
         "7.994369808990873e-05",
         "7.992875713176028e-05",
         "7.991845513147972e-05",
         "7.990262988746735e-05",
         "7.988532818094574e-05",
         "7.98637679649395e-05",
         "7.984975229546331e-05",
         "7.983666580593744e-05",
         "7.982914606468214e-05",
         "7.982207394975311e-05",
         "7.981410989373111e-05",
         "7.97955862153933e-05",
         "7.976094741834126e-05",
         "7.972777600509017e-05",
         "7.971102177420943e-05",
         "7.969571323659057e-05",
         "7.967126751538361e-05"
        ],
        [
         "50%",
         "0.00016358620276425711",
         "0.00016352317452235224",
         "0.00016347434938340983",
         "0.00016342693473279655",
         "0.00016338133799551093",
         "0.00016334892406063726",
         "0.00016332494648783788",
         "0.00016328952825544237",
         "0.00016324643639717293",
         "0.00016320392299703056",
         "0.00016317290886333255",
         "0.0001631336331684274",
         "0.00016312693205673858",
         "0.00016310589398401358",
         "0.00016309138070170844",
         "0.00016303989051837198",
         "0.00016297301120865975",
         "0.00016291765066266893",
         "0.00016286556390674643",
         "0.00016282128285957965",
         "0.0001627625711214399"
        ],
        [
         "75%",
         "0.0003619034578795602",
         "0.0003617250252674084",
         "0.00036157656833388734",
         "0.0003614437348635266",
         "0.00036129204113368525",
         "0.00036118418808615685",
         "0.0003610674377567552",
         "0.0003609677631531221",
         "0.0003608845417126952",
         "0.0003607832561553095",
         "0.00036069756085292887",
         "0.00036056851716881956",
         "0.00036055865972388964",
         "0.00036045935324943864",
         "0.00036039664408009186",
         "0.0003602489510308364",
         "0.00036008010285624596",
         "0.0003599098439884965",
         "0.0003597572016519884",
         "0.0003596322198010772",
         "0.000359505153729439"
        ],
        [
         "max",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176",
         "84.23514653694176"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "      <td>3.133449e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.620279e-04</td>\n",
       "      <td>4.618438e-04</td>\n",
       "      <td>4.617617e-04</td>\n",
       "      <td>4.616339e-04</td>\n",
       "      <td>4.615435e-04</td>\n",
       "      <td>4.615598e-04</td>\n",
       "      <td>4.616128e-04</td>\n",
       "      <td>4.616412e-04</td>\n",
       "      <td>4.616559e-04</td>\n",
       "      <td>4.617216e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>4.617577e-04</td>\n",
       "      <td>4.617911e-04</td>\n",
       "      <td>4.618213e-04</td>\n",
       "      <td>4.621120e-04</td>\n",
       "      <td>4.621884e-04</td>\n",
       "      <td>4.621986e-04</td>\n",
       "      <td>4.663696e-04</td>\n",
       "      <td>4.669277e-04</td>\n",
       "      <td>4.675308e-04</td>\n",
       "      <td>4.683462e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.966728e-02</td>\n",
       "      <td>4.966727e-02</td>\n",
       "      <td>4.966728e-02</td>\n",
       "      <td>4.966728e-02</td>\n",
       "      <td>4.966728e-02</td>\n",
       "      <td>4.966733e-02</td>\n",
       "      <td>4.966738e-02</td>\n",
       "      <td>4.966743e-02</td>\n",
       "      <td>4.966746e-02</td>\n",
       "      <td>4.966759e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>4.966767e-02</td>\n",
       "      <td>4.966769e-02</td>\n",
       "      <td>4.966772e-02</td>\n",
       "      <td>4.966914e-02</td>\n",
       "      <td>4.966940e-02</td>\n",
       "      <td>4.966958e-02</td>\n",
       "      <td>5.018399e-02</td>\n",
       "      <td>5.018789e-02</td>\n",
       "      <td>5.018969e-02</td>\n",
       "      <td>5.019274e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "      <td>-1.021525e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.002706e-05</td>\n",
       "      <td>8.000649e-05</td>\n",
       "      <td>7.998366e-05</td>\n",
       "      <td>7.996174e-05</td>\n",
       "      <td>7.994370e-05</td>\n",
       "      <td>7.992876e-05</td>\n",
       "      <td>7.991846e-05</td>\n",
       "      <td>7.990263e-05</td>\n",
       "      <td>7.988533e-05</td>\n",
       "      <td>7.986377e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.983667e-05</td>\n",
       "      <td>7.982915e-05</td>\n",
       "      <td>7.982207e-05</td>\n",
       "      <td>7.981411e-05</td>\n",
       "      <td>7.979559e-05</td>\n",
       "      <td>7.976095e-05</td>\n",
       "      <td>7.972778e-05</td>\n",
       "      <td>7.971102e-05</td>\n",
       "      <td>7.969571e-05</td>\n",
       "      <td>7.967127e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.635862e-04</td>\n",
       "      <td>1.635232e-04</td>\n",
       "      <td>1.634743e-04</td>\n",
       "      <td>1.634269e-04</td>\n",
       "      <td>1.633813e-04</td>\n",
       "      <td>1.633489e-04</td>\n",
       "      <td>1.633249e-04</td>\n",
       "      <td>1.632895e-04</td>\n",
       "      <td>1.632464e-04</td>\n",
       "      <td>1.632039e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.631336e-04</td>\n",
       "      <td>1.631269e-04</td>\n",
       "      <td>1.631059e-04</td>\n",
       "      <td>1.630914e-04</td>\n",
       "      <td>1.630399e-04</td>\n",
       "      <td>1.629730e-04</td>\n",
       "      <td>1.629177e-04</td>\n",
       "      <td>1.628656e-04</td>\n",
       "      <td>1.628213e-04</td>\n",
       "      <td>1.627626e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.619035e-04</td>\n",
       "      <td>3.617250e-04</td>\n",
       "      <td>3.615766e-04</td>\n",
       "      <td>3.614437e-04</td>\n",
       "      <td>3.612920e-04</td>\n",
       "      <td>3.611842e-04</td>\n",
       "      <td>3.610674e-04</td>\n",
       "      <td>3.609678e-04</td>\n",
       "      <td>3.608845e-04</td>\n",
       "      <td>3.607833e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.605685e-04</td>\n",
       "      <td>3.605587e-04</td>\n",
       "      <td>3.604594e-04</td>\n",
       "      <td>3.603966e-04</td>\n",
       "      <td>3.602490e-04</td>\n",
       "      <td>3.600801e-04</td>\n",
       "      <td>3.599098e-04</td>\n",
       "      <td>3.597572e-04</td>\n",
       "      <td>3.596322e-04</td>\n",
       "      <td>3.595052e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "      <td>8.423515e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature_0     feature_1     feature_2     feature_3     feature_4  \\\n",
       "count  3.133449e+06  3.133449e+06  3.133449e+06  3.133449e+06  3.133449e+06   \n",
       "mean   4.620279e-04  4.618438e-04  4.617617e-04  4.616339e-04  4.615435e-04   \n",
       "std    4.966728e-02  4.966727e-02  4.966728e-02  4.966728e-02  4.966728e-02   \n",
       "min   -1.021525e-02 -1.021525e-02 -1.021525e-02 -1.021525e-02 -1.021525e-02   \n",
       "25%    8.002706e-05  8.000649e-05  7.998366e-05  7.996174e-05  7.994370e-05   \n",
       "50%    1.635862e-04  1.635232e-04  1.634743e-04  1.634269e-04  1.633813e-04   \n",
       "75%    3.619035e-04  3.617250e-04  3.615766e-04  3.614437e-04  3.612920e-04   \n",
       "max    8.423515e+01  8.423515e+01  8.423515e+01  8.423515e+01  8.423515e+01   \n",
       "\n",
       "          feature_5     feature_6     feature_7     feature_8     feature_9  \\\n",
       "count  3.133449e+06  3.133449e+06  3.133449e+06  3.133449e+06  3.133449e+06   \n",
       "mean   4.615598e-04  4.616128e-04  4.616412e-04  4.616559e-04  4.617216e-04   \n",
       "std    4.966733e-02  4.966738e-02  4.966743e-02  4.966746e-02  4.966759e-02   \n",
       "min   -1.021525e-02 -1.021525e-02 -1.021525e-02 -1.021525e-02 -1.021525e-02   \n",
       "25%    7.992876e-05  7.991846e-05  7.990263e-05  7.988533e-05  7.986377e-05   \n",
       "50%    1.633489e-04  1.633249e-04  1.632895e-04  1.632464e-04  1.632039e-04   \n",
       "75%    3.611842e-04  3.610674e-04  3.609678e-04  3.608845e-04  3.607833e-04   \n",
       "max    8.423515e+01  8.423515e+01  8.423515e+01  8.423515e+01  8.423515e+01   \n",
       "\n",
       "       ...    feature_11    feature_12    feature_13    feature_14  \\\n",
       "count  ...  3.133449e+06  3.133449e+06  3.133449e+06  3.133449e+06   \n",
       "mean   ...  4.617577e-04  4.617911e-04  4.618213e-04  4.621120e-04   \n",
       "std    ...  4.966767e-02  4.966769e-02  4.966772e-02  4.966914e-02   \n",
       "min    ... -1.021525e-02 -1.021525e-02 -1.021525e-02 -1.021525e-02   \n",
       "25%    ...  7.983667e-05  7.982915e-05  7.982207e-05  7.981411e-05   \n",
       "50%    ...  1.631336e-04  1.631269e-04  1.631059e-04  1.630914e-04   \n",
       "75%    ...  3.605685e-04  3.605587e-04  3.604594e-04  3.603966e-04   \n",
       "max    ...  8.423515e+01  8.423515e+01  8.423515e+01  8.423515e+01   \n",
       "\n",
       "         feature_15    feature_16    feature_17    feature_18    feature_19  \\\n",
       "count  3.133449e+06  3.133449e+06  3.133449e+06  3.133449e+06  3.133449e+06   \n",
       "mean   4.621884e-04  4.621986e-04  4.663696e-04  4.669277e-04  4.675308e-04   \n",
       "std    4.966940e-02  4.966958e-02  5.018399e-02  5.018789e-02  5.018969e-02   \n",
       "min   -1.021525e-02 -1.021525e-02 -1.021525e-02 -1.021525e-02 -1.021525e-02   \n",
       "25%    7.979559e-05  7.976095e-05  7.972778e-05  7.971102e-05  7.969571e-05   \n",
       "50%    1.630399e-04  1.629730e-04  1.629177e-04  1.628656e-04  1.628213e-04   \n",
       "75%    3.602490e-04  3.600801e-04  3.599098e-04  3.597572e-04  3.596322e-04   \n",
       "max    8.423515e+01  8.423515e+01  8.423515e+01  8.423515e+01  8.423515e+01   \n",
       "\n",
       "                  y  \n",
       "count  3.133449e+06  \n",
       "mean   4.683462e-04  \n",
       "std    5.019274e-02  \n",
       "min   -1.021525e-02  \n",
       "25%    7.967127e-05  \n",
       "50%    1.627626e-04  \n",
       "75%    3.595052e-04  \n",
       "max    8.423515e+01  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb4aaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['window'], inplace=True)\n",
    "#df['y'] = df['y'].apply(lambda x: 1 if x >= 0 else 0 if x < 0 else 0)\n",
    "train_data = df.iloc[:100000].copy()\n",
    "test_data = df.iloc[100001:150000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "418e2509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;20m17:17:04 - WFCVGenerator - DEBUG - WFConfig(train=753d, val=251d, test=251d, lags=20, step=251, max_folds=None) (walkforward.py:5)\u001b[0m\n",
      "\u001b[34;20m17:17:04 - WFCVGenerator - DEBUG - Loading data via preprocess() (walkforward.py:47)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/home/002/corte911/code/finNN_code/src/pipeline/preprocessing.py:171: RuntimeWarning: invalid value encountered in log\n",
      "  log_co = np.log(close / open_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nans in var: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;20m17:17:06 - WFCVGenerator - DEBUG - Loaded df: shape=(3155297, 3) dtypes={'t': dtype('int16'), 'var': dtype('float64'), 'permno': Int64Dtype()} head=\n",
      " t      var  permno\n",
      " 0 0.001527   10078\n",
      " 1 0.001098   10078\n",
      " 2 0.004587   10078 (walkforward.py:69)\u001b[0m\n",
      "\u001b[34;20m17:17:06 - WFCVGenerator - DEBUG - Predicting var (walkforward.py:12)\u001b[0m\n",
      "\u001b[34;20m17:17:06 - WFCVGenerator - DEBUG - self.df.columns: Index(['t', 'var', 'permno'], dtype='object') (walkforward.py:13)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.0062\n",
      "Epoch [20/50], Loss: 0.0051\n",
      "Epoch [30/50], Loss: 0.0085\n",
      "Epoch [40/50], Loss: 0.0049\n",
      "Epoch [50/50], Loss: 0.0096\n",
      "Training finished.\n",
      "Test MSE Loss: 0.0080\n",
      "\n",
      "Sample Predictions vs Actuals:\n",
      "Prediction: -8.34, Actual: 0.00\n",
      "Prediction: -8.18, Actual: 0.00\n",
      "Prediction: -7.20, Actual: 0.00\n",
      "Prediction: -7.41, Actual: 0.00\n",
      "Prediction: -8.07, Actual: 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cfg = AppConfig.from_dict(f\"{CONFIG_DIR}/debug.yaml\")\n",
    "wf = WFCVGenerator(config=cfg.walkforward)\n",
    "\n",
    "train_df = train_data\n",
    "test_df = test_data\n",
    "\n",
    "X_train_df = train_df.drop('y', axis=1)\n",
    "y_train_df = train_df['y']\n",
    "X_test_df = test_df.drop('y', axis=1)\n",
    "y_test_df = test_df['y']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_df.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_df.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# --- 3. Define the Neural Network ---\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# --- 4. Training ---\n",
    "# Instantiate the model\n",
    "input_features = X_train.shape[1]\n",
    "model = SimpleNet(input_size=input_features)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 5. Evaluation ---\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():  # No need to track gradients for evaluation\n",
    "    predictions = model(X_test)\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test MSE Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# You can inspect some predictions\n",
    "print(\"\\nSample Predictions vs Actuals:\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {predictions[i].item():.2f}, Actual: {y_test[i].item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e8129b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [10/50], Loss: 0.0011\n",
      "Epoch [20/50], Loss: 0.0004\n",
      "Epoch [30/50], Loss: 0.0009\n",
      "Epoch [40/50], Loss: 0.0007\n",
      "Epoch [50/50], Loss: 0.0019\n",
      "Training finished.\n",
      "Test MSE Loss: 0.0015\n",
      "\n",
      "Sample Predictions vs Actuals:\n",
      "Prediction: 0.00, Actual: -0.04\n",
      "Prediction: 0.00, Actual: 0.03\n",
      "Prediction: 0.00, Actual: 0.02\n",
      "Prediction: 0.00, Actual: 0.01\n",
      "Prediction: 0.00, Actual: 0.05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "gpu_test()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_df.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_df.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_train = X_train_tensor.unsqueeze(1)\n",
    "X_test = X_test_tensor.unsqueeze(1)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# --- 3. Define the CNN ---\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Input shape: (batch_size, 1, num_features)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # The size of the flattened layer will be out_channels * num_features\n",
    "        self.fc1 = nn.Linear(16 * num_features, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1, num_features)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        # x shape: (batch_size, 16, num_features)\n",
    "        x = self.flatten(x)\n",
    "        # x shape: (batch_size, 16 * num_features)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# --- 4. Training ---\n",
    "# Instantiate the model\n",
    "input_features = X_train.shape[2] # Number of features is the last dimension\n",
    "model = SimpleCNN(num_features=input_features)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 5. Evaluation ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test MSE Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# You can inspect some predictions\n",
    "print(\"\\nSample Predictions vs Actuals:\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {predictions[i].item():.2f}, Actual: {y_test[i].item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
